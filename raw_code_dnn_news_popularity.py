# -*- coding: utf-8 -*-
"""Winter 2019 GLIS 689 News Popularity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gD3-8ON2cgUEJvdvM1PI8t3OJiZ4EFP6
"""

import pandas
import numpy
import pickle
from matplotlib import pyplot
import tensorflow
import random
import os
os.environ['PYTHONHASHSEED'] = '0'
numpy.random.seed(1)
random.seed(1)
session_conf = tensorflow.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
from keras import backend as K
tensorflow.set_random_seed(1)
sess = tensorflow.Session(graph=tensorflow.get_default_graph(), config=session_conf)
K.set_session(sess)

"""## Throwing the articles into Universal Sentence Encoder"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/open?id=1ks6t2AMeipraYf1yhgjuDcbaWNxRieB3'
fluff, id = link.split('=')
print (id)

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('og_dataset.csv')  
og_dataset = pandas.read_csv('og_dataset.csv')

import tensorflow as tf
import tensorflow_hub as hub

def embed_useT(module):
    with tf.Graph().as_default():
        sentences = tf.placeholder(tf.string)
        embed = hub.Module(module)
        embeddings = embed(sentences)
        session = tf.train.MonitoredSession()
    return lambda x: session.run(embeddings, {sentences: x})

og_dataset = og_dataset.drop(columns=["Unnamed: 0"],axis=1)

og_dataset["vec"] = numpy.nan
og_dataset["vec"] = og_dataset["vec"].astype(object)

embed_fn = embed_useT("https://tfhub.dev/google/universal-sentence-encoder/2")

for i in range(len(og_dataset)):
    try:
        og_dataset.at[i,"vec"] = embed_fn([og_dataset.iloc[i,0]])
    except:
        og_dataset.at[i,"vec"] = numpy.nan

from google.colab import drive
drive.mount('drive')
og_dataset.to_csv('og_dataset_vec.csv')
!cp og_dataset_vec.csv drive/My\ Drive/Winter_2019_GLIS_689
og_dataset.to_pickle('og_dataset_vec.pkl')
!cp og_dataset_vec.pkl drive/My\ Drive/Winter_2019_GLIS_689

"""## Getting data ready"""

from google.colab import drive
drive.mount('drive')

og_dataset = pandas.read_pickle("drive/My Drive/Winter_2019_GLIS_689/og_dataset_vec.pkl")
#(39494,62)

# removing NAs
og_dataset = og_dataset.drop(og_dataset[og_dataset["vec"].isnull()].index)
og_dataset = og_dataset.drop(og_dataset[og_dataset[" shares"].isnull()].index)

# removing outliers based on z score
#og_dataset = og_dataset[numpy.abs(og_dataset[" shares"]-og_dataset[" shares"].mean())<=(3*og_dataset[" shares"].std())]
#(39188,62)

#removing outliers based on IQR score
def remove_outlier(df_in, col_name):
    q1 = df_in[col_name].quantile(0.25)
    q3 = df_in[col_name].quantile(0.75)
    iqr = q3-q1 #Interquartile range
    fence_low  = q1-1.5*iqr
    fence_high = q3+1.5*iqr
    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]
    return df_out
og_dataset = remove_outlier(og_dataset, " shares")
#(34968,62)
#(32669,62)
#(31713,62)
#(30905,62)
#(30253,62) # Applied 5 times

import seaborn as sns
sns.boxplot(x=og_dataset[' shares'])

og_dataset[" shares"].plot(kind='density')

# Setting X and y
frames = [pandas.DataFrame(og_dataset["vec"].values[i]) for i in range(og_dataset.shape[0])]
X = pandas.concat(frames)
X = X.reset_index(drop=True)
y = og_dataset[" shares"].values

"""### Subsetting the dataset by its article categories"""

og_lifestyle = og_dataset[og_dataset[" data_channel_is_lifestyle"]==1]
og_entertainment = og_dataset[og_dataset[" data_channel_is_entertainment"]==1]
og_bus = og_dataset[og_dataset[" data_channel_is_bus"]==1]
og_socmed = og_dataset[og_dataset[" data_channel_is_socmed"]==1]
og_tech = og_dataset[og_dataset[" data_channel_is_tech"]==1]
og_world = og_dataset[og_dataset[" data_channel_is_world"]==1]

print(og_lifestyle.shape)
print(og_entertainment.shape)
print(og_bus.shape)
print(og_socmed.shape)
print(og_tech.shape)
print(og_world.shape)

sns.boxplot(x=og_lifestyle[' shares'])

sns.boxplot(x=og_entertainment[' shares'])

sns.boxplot(x=og_bus[' shares'])

sns.boxplot(x=og_socmed[' shares'])

sns.boxplot(x=og_tech[' shares'])

sns.boxplot(x=og_world[' shares'])

"""##Building Keras Model"""

from keras.models import Sequential
from keras.layers import Input, Dense, Dropout
from keras.layers import Flatten
from keras.models import Model
from keras.optimizers import Adam, SGD

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)

pca = PCA(n_components=500)
pca.fit(X_train)

pyplot.plot(numpy.cumsum(pca.explained_variance_ratio_))
pyplot.xlabel('Number of components')
pyplot.ylabel('Cumulative explained variance')

# Applying dimensitonality reduction with PCA
pca = PCA(n_components=100)

X_train = pca.fit_transform(X_train)
X_val = pca.transform(X_val)
X_test = pca.transform(X_test)
pca_std = numpy.std(X_train)

print(X_train.shape)

# Scaling the data with MinMaxScaler
x_standardizer = MinMaxScaler()
y_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
y_std = y_standardizer.fit_transform(y_train.reshape(-1,1))
X_val_std = x_standardizer.transform(X_val)
y_val_std = y_standardizer.transform(y_val.reshape(-1,1))
X_test_std = x_standardizer.transform(X_test)
y_test_std = y_standardizer.transform(y_test.reshape(-1,1))

model = Sequential()
model.add(Dense(100, activation='relu',input_shape=(100,)))
#model.add(Dropout(0.5))
model.add(Dense(100, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(100, activation='relu'))
#model.add(Dropout(0.5))
#model.add(Dense(100, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))
print(model.summary())
adam = Adam(lr=0.0001)
sgd = SGD(lr=0.001)
model.compile(loss='mse', optimizer=adam)

temp_history = model.fit(X_std, y_train, epochs=50, batch_size=32, validation_data=(X_val_std, y_val))

pyplot.plot(temp_history.history['loss'])
pyplot.plot(temp_history.history['val_loss'])
pyplot.title('model train vs validation loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.legend(['train', 'validation'], loc='upper right')
pyplot.show()

y_test_pred = model.predict(X_test_std)
print(numpy.sqrt(mean_squared_error(y_test, y_test_pred)))
mean_absolute_error(y_test,y_test_pred)

lir = LinearRegression()
lirmodel = lir.fit(X_std, y_train)
y_test_pred = lirmodel.predict(X_test_std)

#mse = mean_squared_error(y_test, y_standardizer.inverse_transform(y_test_pred))
mse = mean_squared_error(y_test, y_test_pred)
print(numpy.sqrt(mse))

rfr = RandomForestRegressor()
rfrmodel = rfr.fit(X_std, y_train)
y_test_pred = rfrmodel.predict(X_test_std)

#mse = mean_squared_error(y_test, y_standardizer.inverse_transform(y_test_pred))
mse = mean_squared_error(y_test, y_test_pred)
print(numpy.sqrt(mse))

temp = pandas.DataFrame(y_test_pred)
temp.plot(kind='density')

temp = pandas.DataFrame(y_test, columns=["hello"]).hello.unique()
sns.boxplot(x=temp)

pandas.DataFrame(y_test).plot(kind="density")

"""## Converting it to a classification problem"""

og_dataset_classify = og_dataset.copy()

og_dataset_classify["class_target"] = numpy.where(og_dataset_classify[" shares"]>1313, "overaverage", "belowaverage")
#og_dataset_classify["class_target"] = numpy.where(og_dataset_classify[" shares"]>1700, "fourth", numpy.where(og_dataset_classify[" shares"]>1200, "third",
#                                                                                                            numpy.where(og_dataset_classify[" shares"]>858, "second", "first")))

og_dataset_classify.loc[:,[" shares","class_target"]]

from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

X_classify = X.copy()
y_classify = og_dataset_classify["class_target"].values

encoder = LabelEncoder()
encoder.fit(y_classify)
y_classify = encoder.transform(y_classify)
# convert integers to dummy variables (i.e. one hot encoded)
y_classify = np_utils.to_categorical(y_classify)

X_train, X_test, y_train, y_test = train_test_split(X_classify,y_classify,test_size=0.3,random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_test_std = x_standardizer.transform(X_test)

model_2 = Sequential()
model_2.add(Dense(512, activation='relu',input_shape=(512,)))
model_2.add(Dropout(0.5))
#model_2.add(Dense(256, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(128, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(64, activation='relu'))
#model_2.add(Dropout(0.5))
model_2.add(Dense(2, activation='softmax'))
print(model_2.summary())
adam = Adam(lr=0.0001)
model_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

temp_history_classify = model_2.fit(X_train, y_train, epochs=50, validation_split=0.2)

model_2.evaluate(X_test, y_test)
# loss and accuracy will come out

y_test_pred=model_2.predict(X_test)
y_test_pred =(y_test_pred>0.5)

confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))

"""## With other variables"""

og_dataset.describe()

X_extend = og_dataset.loc[:,[" n_tokens_title"," n_tokens_content"," num_imgs"," num_videos"," average_token_length",
                             " data_channel_is_lifestyle"," data_channel_is_entertainment"," data_channel_is_bus",
                            " data_channel_is_socmed"," data_channel_is_tech"," data_channel_is_world"," weekday_is_monday",
                            " weekday_is_tuesday"," weekday_is_wednesday"," weekday_is_thursday"," weekday_is_friday"," weekday_is_saturday",
                            " weekday_is_sunday"," is_weekend"," global_subjectivity"," global_sentiment_polarity"," title_subjectivity",
                            " title_sentiment_polarity"]]

X_extend.head()

X_extend = pandas.concat([X, X_extend.reset_index(drop=True)], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X_extend,y,test_size=0.3,random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)

X_train, X_test, y_train, y_test = train_test_split(X_nonvec,y,test_size=0.3,random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_val_std = x_standardizer.transform(X_val)
X_test_std = x_standardizer.transform(X_test)

model = Sequential()
model.add(Dense(535, activation='relu',input_shape=(535,)))
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
#model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.5))
#model.add(Dense(64, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))
print(model.summary())
adam = Adam(lr=0.0001)
model.compile(loss='mse', optimizer=adam)

temp_history = model.fit(X_std, y_train, epochs=30, batch_size=32, validation_data=(X_val_std, y_val))

pyplot.plot(temp_history.history['loss'])
pyplot.plot(temp_history.history['val_loss'])
pyplot.title('model train vs validation loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.legend(['train', 'validation'], loc='upper right')
pyplot.show()

y_test_pred = model.predict(X_test_std)
print(numpy.sqrt(mean_squared_error(y_test, y_test_pred)))
mean_absolute_error(y_test,y_test_pred)

X_classify = X_extend.copy()

X_train, X_test, y_train, y_test = train_test_split(X_classify,y_classify,test_size=0.3,random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_test_std = x_standardizer.transform(X_test)

model_2 = Sequential()
model_2.add(Dense(535, activation='relu',input_shape=(535,)))
model_2.add(Dropout(0.5))
#model_2.add(Dense(256, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(128, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(64, activation='relu'))
#model_2.add(Dropout(0.5))
model_2.add(Dense(2, activation='softmax'))
print(model_2.summary())
adam = Adam(lr=0.0001)
model_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

temp_history_classify = model_2.fit(X_train, y_train, epochs=50, validation_split=0.2)

model_2.evaluate(X_test, y_test)
# loss and accuracy will come out

y_test_pred=model_2.predict(X_test)
y_test_pred =(y_test_pred>0.5)
confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))

"""## How about applying feature selection? (with LASSO)"""

from sklearn.linear_model import Lasso
lassomodel = Lasso(alpha=0.1, positive=True) # alpha here is the penalty term ?????
lassomodel.fit(X,y)
temp = pandas.DataFrame(list(zip(X.columns,lassomodel.coef_)), columns=['predictor','coefficient'])
temp = temp.sort_values(by='coefficient',ascending=False)

print(temp.iloc[0:50,:])

X_lassoed = X.iloc[:,[339,21,250,31,106,279,482,235,89,329,219,307,258,338,5
                      ,422,402,321,317,57,288,
                    92,486,403,228,162,133,41,438,413,2]]
#30253,31
#30253,15

X_train, X_test, y_train, y_test = train_test_split(X_nonvec,y,test_size=0.3,random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_val_std = x_standardizer.transform(X_val)
X_test_std = x_standardizer.transform(X_test)

model = Sequential()
model.add(Dense(31, activation='linear',input_shape=(31,)))
#model.add(Dropout(0.5))
#model.add(Dense(256, activation='relu'))
#model.add(Dropout(0.5))
#model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.5))
#model.add(Dense(64, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))
print(model.summary())
adam = Adam(lr=0.0001)
model.compile(loss='mse', optimizer=adam)

temp_history = model.fit(X_std, y_train, epochs=30, batch_size=32, validation_data=(X_val_std, y_val))

pyplot.plot(temp_history.history['loss'])
pyplot.plot(temp_history.history['val_loss'])
pyplot.title('model train vs validation loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.legend(['train', 'validation'], loc='upper right')
pyplot.show()

y_test_pred = model.predict(X_test_std)
print(numpy.sqrt(mean_squared_error(y_test, y_test_pred)))
mean_absolute_error(y_test,y_test_pred)

X_classify = X_lassoed.copy()

X_train, X_test, y_train, y_test = train_test_split(X_classify,y_classify,test_size=0.3,random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_test_std = x_standardizer.transform(X_test)

model_2 = Sequential()
model_2.add(Dense(31, activation='relu',input_shape=(31,)))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(256, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(128, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(64, activation='relu'))
#model_2.add(Dropout(0.5))
model_2.add(Dense(2, activation='softmax'))
print(model_2.summary())
adam = Adam(lr=0.0001)
model_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

temp_history_classify = model_2.fit(X_train, y_train, epochs=50, validation_split=0.2)

model_2.evaluate(X_test, y_test)
# loss and accuracy will come out

"""## Feature selection with other features"""

lassomodel = Lasso(alpha=0.1, positive=True) # alpha here is the penalty term
lassomodel.fit(X_extend,y)
temp = pandas.DataFrame(list(zip(X_extend.columns,lassomodel.coef_)), columns=['predictor','coefficient'])
temp = temp.sort_values(by='coefficient',ascending=False)

X_extend_lassoed = X_extend.loc[:,[" data_channel_is_socmed"," is_weekend"," data_channel_is_tech",250,5,307,279," data_channel_is_lifestyle",
                                  219, 31, " data_channel_is_bus", 339, " global_sentiment_polarity", 403, 235, 158, 89, 422, 252, 261, 56, 162]]

X_train, X_test, y_train, y_test = train_test_split(X_nonvec,y,test_size=0.3,random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_val_std = x_standardizer.transform(X_val)
X_test_std = x_standardizer.transform(X_test)

model = Sequential()
model.add(Dense(22, activation='linear',input_shape=(22,)))
#model.add(Dropout(0.5))
#model.add(Dense(256, activation='relu'))
#model.add(Dropout(0.5))
#model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.5))
#model.add(Dense(64, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))
print(model.summary())
adam = Adam(lr=0.00001)
model.compile(loss='mse', optimizer=adam)

temp_history = model.fit(X_std, y_train, epochs=30, batch_size=32, validation_data=(X_val_std, y_val))

pyplot.plot(temp_history.history['loss'])
pyplot.plot(temp_history.history['val_loss'])
pyplot.title('model train vs validation loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.legend(['train', 'validation'], loc='upper right')
pyplot.show()

y_test_pred = model.predict(X_test_std)
print(numpy.sqrt(mean_squared_error(y_test, y_test_pred)))
mean_absolute_error(y_test,y_test_pred)

X_classify = X_extend_lassoed.copy()

X_train, X_test, y_train, y_test = train_test_split(X_classify,y_classify,test_size=0.3,random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_test_std = x_standardizer.transform(X_test)

model_2 = Sequential()
model_2.add(Dense(22, activation='relu',input_shape=(22,)))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(256, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(128, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(64, activation='relu'))
#model_2.add(Dropout(0.5))
model_2.add(Dense(2, activation='softmax'))
print(model_2.summary())
adam = Adam(lr=0.0001)
model_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

temp_history_classify = model_2.fit(X_train, y_train, epochs=50, validation_split=0.2)

model_2.evaluate(X_test, y_test)
# loss and accuracy will come out

"""## Only original columns"""

X_nonvec = og_dataset.loc[:,[" n_tokens_title"," n_tokens_content"," num_imgs"," num_videos"," average_token_length",
                             " data_channel_is_lifestyle"," data_channel_is_entertainment"," data_channel_is_bus",
                            " data_channel_is_socmed"," data_channel_is_tech"," data_channel_is_world"," is_weekend"," global_subjectivity"," global_sentiment_polarity"," title_subjectivity",
                            " title_sentiment_polarity"]]
#" weekday_is_monday"," weekday_is_tuesday"," weekday_is_wednesday"," weekday_is_thursday"," weekday_is_friday"," weekday_is_saturday"," weekday_is_sunday",

X_train, X_test, y_train, y_test = train_test_split(X_nonvec,y,test_size=0.3,random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_val_std = x_standardizer.transform(X_val)
X_test_std = x_standardizer.transform(X_test)

model = Sequential()
model.add(Dense(16, activation='relu',input_shape=(16,)))
#model.add(Dropout(0.5))
model.add(Dense(16, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(16, activation='relu'))
#model.add(Dropout(0.5))
#model.add(Dense(64, activation='relu'))
#model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))
print(model.summary())
adam = Adam(lr=0.0001)
model.compile(loss='mse', optimizer=adam)

temp_history = model.fit(X_std, y_train, epochs=50, batch_size=32, validation_data=(X_val_std, y_val))

pyplot.plot(temp_history.history['loss'])
pyplot.plot(temp_history.history['val_loss'])
pyplot.title('model train vs validation loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.legend(['train', 'validation'], loc='upper right')
pyplot.show()

y_test_pred = model.predict(X_test_std)
print(numpy.sqrt(mean_squared_error(y_test, y_test_pred)))
mean_absolute_error(y_test,y_test_pred)

X_classify = X_nonvec.copy()
X_train, X_test, y_train, y_test = train_test_split(X_classify,y_classify,test_size=0.2,random_state=5)
x_standardizer = MinMaxScaler()
X_std = x_standardizer.fit_transform(X_train)
X_test_std = x_standardizer.transform(X_test)

model_2 = Sequential()
model_2.add(Dense(23, activation='relu',input_shape=(23,)))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(256, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(128, activation='relu'))
#model_2.add(Dropout(0.5))
#model_2.add(Dense(64, activation='relu'))
#model_2.add(Dropout(0.5))
model_2.add(Dense(2, activation='sigmoid'))
print(model_2.summary())
adam = Adam(lr=0.0001)
model_2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

temp_history_classify = model_2.fit(X_std, y_train, epochs=50, validation_split=0.3)

model_2.evaluate(X_test_std, y_test)
# loss and accuracy will come out